{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData_Tarjetas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOzUxlf1h-vW",
        "outputId": "b9f942b5-9004-4b09-b82b-052a5486746c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsRIzMYHjJoc"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sccq8pNq9v2L",
        "outputId": "b0a68263-5338-4cc8-cbb4-c5599610c4d2"
      },
      "source": [
        "pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 60.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=d69d0d63211363181a71e0906f5c15db3abc695242dfdde669945b4c90f325be\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "ytWFGyKT9dQx",
        "outputId": "5dec0053-d035-4376-de5f-54e8001be538"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder.appName('Dataframe').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://0d02b223a051:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Dataframe</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc4c0653b90>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5seau4j-WRC",
        "outputId": "d7d34522-94ab-4972-89ae-ca61a5324cec"
      },
      "source": [
        "## read the dataset\n",
        "df_cards=spark.read.option('header','true').option('delimiter','|').csv('/content/drive/MyDrive/DatosCollabBigData/cards.csv',inferSchema=True)\n",
        "df_cards.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+------------+----------+--------------+-------+------+\n",
            "|CP_CLIENTE|CP_COMERCIO|      SECTOR|       DIA|FRANJA_HORARIA|IMPORTE|NUM_OP|\n",
            "+----------+-----------+------------+----------+--------------+-------+------+\n",
            "|      4007|       4006|ALIMENTACION|2015-09-05|         08-10| 297.05|    10|\n",
            "|      4007|       4006|ALIMENTACION|2015-04-20|         14-16| 494.23|    17|\n",
            "|      4007|       4006|ALIMENTACION|2015-06-04|         10-12|1694.15|    57|\n",
            "|      4002|       4009|ALIMENTACION|2015-12-05|         20-22|  79.71|     2|\n",
            "|      4002|       4007|ALIMENTACION|2015-03-07|         20-22| 181.28|     3|\n",
            "+----------+-----------+------------+----------+--------------+-------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j55sl47h_dhK",
        "outputId": "5a110bd4-c3c6-442e-cf7e-f429e1260108"
      },
      "source": [
        "df_weather=spark.read.option('header','true').option('delimiter',';').csv('/content/drive/MyDrive/DatosCollabBigData/weather.csv',inferSchema=True)\n",
        "df_weather.show(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---+----+-----+----+-----+----+------+------+------+---------+---------+----+------+----+\n",
            "|     FECHA|DIA|TMax|HTMax|TMin|HTMin|TMed|HumMax|HumMin|HumMed|VelViento|DirViento| Rad|Precip| ETo|\n",
            "+----------+---+----+-----+----+-----+----+------+------+------+---------+---------+----+------+----+\n",
            "|2015-12-30|365|19.2|12:02| 8.6| 4:06|13.9|  98.8|  49.3|  71.4|      0.6|    337.2| 9.9|   0.0|1.04|\n",
            "|2015-12-29|363|18.8|12:38|10.4| 4:32|14.2| 100.0|  64.7|  81.2|      1.0|    292.7| 8.5|   0.0|1.07|\n",
            "|2015-12-29|364|18.8|12:14|11.0|22:22|14.4|  89.3|  55.6|  70.2|      1.2|    328.2|10.0|   0.0|1.32|\n",
            "|2015-12-28|362|18.9|10:00|11.6|23:48|15.8|  89.0|  47.4|  66.1|      1.2|    280.8| 9.3|   0.0|1.38|\n",
            "|2015-12-27|361|19.6|13:48|12.5| 6:30|15.8|  72.9|  48.3|  61.1|      2.0|     35.8| 9.4|   0.0|1.98|\n",
            "+----------+---+----+-----+----+-----+----+------+------+------+---------+---------+----+------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nvo2XYvBmVm",
        "outputId": "9d4633b3-c473-4816-a2fa-3a737c7e03b8"
      },
      "source": [
        "df_cards.printSchema()\n",
        "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CP_CLIENTE: integer (nullable = true)\n",
            " |-- CP_COMERCIO: integer (nullable = true)\n",
            " |-- SECTOR: string (nullable = true)\n",
            " |-- DIA: string (nullable = true)\n",
            " |-- FRANJA_HORARIA: string (nullable = true)\n",
            " |-- IMPORTE: double (nullable = true)\n",
            " |-- NUM_OP: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWpvLsm3mygX"
      },
      "source": [
        "df_cards.createOrReplaceTempView(\"TCARDS\")\n",
        "df_weather.createOrReplaceTempView(\"TWEATHER\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwzMj0KnED0h"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87H7DpPkYPk6"
      },
      "source": [
        "**Propuestas de consultas:**\n",
        "* Días de sol con gastos\n",
        "* Media de gastos por sector\n",
        "* Meses de gastos\n",
        "* Horas de mayor gasto en verano\n",
        "* Compras en día de lluvia\n",
        "* Compras con temperaturas extremas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-v84R3ThCVC"
      },
      "source": [
        "**Creación de Dataframe para el JOIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93FAsaNyhBj0"
      },
      "source": [
        "df_conteo = spark.sql(\"SELECT DIA, count(DIA) AS `ENTRADAS DE DATOS`, SUM(IMPORTE) AS `GASTOS` FROM TCARDS GROUP BY DIA ORDER BY DIA;\")\n",
        "df_conteo.createOrReplaceTempView(\"TDIAS\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z5Lscj4ePHM"
      },
      "source": [
        "**Pruebas para los join**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fmdyVgKkmOc",
        "outputId": "799a7674-85d9-45da-f4ad-d6ad36fd83d7"
      },
      "source": [
        "df_cards.select('SECTOR', 'IMPORTE').groupby('SECTOR').sum('IMPORTE').show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+--------------------+\n",
            "|             SECTOR|        sum(IMPORTE)|\n",
            "+-------------------+--------------------+\n",
            "|              HOGAR|    5811188.79000107|\n",
            "|               AUTO|   2416929.799999998|\n",
            "|       ALIMENTACION|2.8270662249997895E7|\n",
            "|MODA Y COMPLEMENTOS|2.6734983769985665E7|\n",
            "|            BELLEZA|  3187440.1600001054|\n",
            "|       RESTAURACION|   5194634.310000212|\n",
            "|              OTROS|  6954260.5400012825|\n",
            "|              SALUD|1.1072741890000245E7|\n",
            "|OCIO Y TIEMPO LIBRE|   4863853.710002758|\n",
            "|         TECNOLOGIA|   3723798.409999931|\n",
            "+-------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FzhwNh1bCS8"
      },
      "source": [
        "**Gastos por sector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yy42bVbbJBD"
      },
      "source": [
        "dfsectores = spark.sql(\"SELECT SECTOR, ROUND(SUM(IMPORTE), 2) AS `GASTO TOTAL` FROM TCARDS GROUP BY SECTOR;\")\n",
        "dfsectores = dfsectores.toPandas()\n",
        "sectores_json = dfsectores.to_json()\n",
        "sectores_json = json.loads(sectores_json)\n",
        "sectores_json\n",
        "\n",
        "with open('/content/drive/MyDrive/DatosCollabBigData/sectores.json', 'w') as file:\n",
        "    json.dump(sectores_json, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCh94vZtbJO0"
      },
      "source": [
        "**Meses de gastos**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehIPdTnpbMZq"
      },
      "source": [
        "dfgastosMonth = spark.sql(\"SELECT MONTH(DIA) AS `MES`, ROUND(SUM(IMPORTE), 2) AS `IMPORTES` FROM TCARDS GROUP BY `MES` ORDER BY `MES`;\")\n",
        "dfmeses = dfgastosMonth.toPandas()\n",
        "meses_json = dfmeses.to_json()\n",
        "meses_json = json.loads(meses_json)\n",
        "meses_json\n",
        "\n",
        "with open('/content/drive/MyDrive/DatosCollabBigData/meses.json', 'w') as file:\n",
        "    json.dump(meses_json, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2W93_WYbMtV"
      },
      "source": [
        "**Horas de mayor gasto en verano**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK6Un9pqbRVr"
      },
      "source": [
        "dfgastosSummer = spark.sql(\"SELECT FRANJA_HORARIA, ROUND(SUM(IMPORTE), 2) AS `IMPORTE TOTAL` FROM TCARDS WHERE MONTH(DIA) BETWEEN 6 AND 8 GROUP BY FRANJA_HORARIA ORDER BY FRANJA_HORARIA ASC;\")\n",
        "dfsummer = dfgastosSummer.toPandas()\n",
        "summer_json = dfsummer.to_json()\n",
        "summer_json = json.loads(summer_json)\n",
        "summer_json\n",
        "\n",
        "with open('/content/drive/MyDrive/DatosCollabBigData/summer.json', 'w') as file:\n",
        "    json.dump(summer_json, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd8zpHRHqqtF"
      },
      "source": [
        "**Horas de mayor gasto en invierno**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPqwVB48qv1x"
      },
      "source": [
        "dfgastosWinter = spark.sql(\"SELECT FRANJA_HORARIA, ROUND(SUM(IMPORTE), 2) AS `IMPORTE TOTAL` FROM TCARDS WHERE MONTH(DIA) BETWEEN 1 AND 2 OR MONTH(DIA) = 12 GROUP BY FRANJA_HORARIA ORDER BY FRANJA_HORARIA ASC;\")\n",
        "dfwinter = dfgastosWinter.toPandas()\n",
        "winter_json = dfwinter.to_json()\n",
        "winter_json = json.loads(winter_json)\n",
        "winter_json\n",
        "\n",
        "with open('/content/drive/MyDrive/DatosCollabBigData/winter.json', 'w') as file:\n",
        "    json.dump(winter_json, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7D-LHvxbWxp"
      },
      "source": [
        "**Gastos con temperaturas altas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMu41go2bbQ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "fbb688a5-d077-4f98-8065-c9436d42498c"
      },
      "source": [
        "dfgastosTAltas = spark.sql(\"SELECT TDIAS.DIA, TWEATHER.TMed, ROUND(TDIAS.GASTOS, 2) AS `GASTOS` FROM TDIAS JOIN TWEATHER ON TDIAS.DIA = TWEATHER.FECHA WHERE TWEATHER.TMed >= 27 GROUP BY TDIAS.DIA, TWEATHER.TMed, TDIAS.GASTOS ORDER BY TDIAS.DIA;\")\n",
        "dftaltas = dfgastosTAltas.toPandas()\n",
        "taltas_json = dftaltas.to_json()\n",
        "taltas_json = json.loads(taltas_json)\n",
        "taltas_json\n",
        "\n",
        "with open('/content/drive/MyDrive/DatosCollabBigData/taltas.json', 'w') as file:\n",
        "    json.dump(taltas_json, file, indent=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-41ca3e2263c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfgastosTAltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT TDIAS.DIA, TWEATHER.TMed, ROUND(TDIAS.GASTOS, 2) AS `GASTOS` FROM TDIAS JOIN TWEATHER ON TDIAS.DIA = TWEATHER.FECHA WHERE TWEATHER.TMed >= 27 GROUP BY TDIAS.DIA, TWEATHER.TMed, TDIAS.GASTOS ORDER BY TDIAS.DIA;\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdftaltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfgastosTAltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtaltas_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdftaltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtaltas_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaltas_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtaltas_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \"\"\"\n\u001b[0;32m--> 723\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: TDIAS; line 1 pos 73;\n'Sort ['TDIAS.DIA ASC NULLS FIRST], true\n+- 'Aggregate ['TDIAS.DIA, 'TWEATHER.TMed, 'TDIAS.GASTOS], ['TDIAS.DIA, 'TWEATHER.TMed, 'ROUND('TDIAS.GASTOS, 2) AS GASTOS#227]\n   +- 'Filter ('TWEATHER.TMed >= 27)\n      +- 'Join Inner, ('TDIAS.DIA = 'TWEATHER.FECHA)\n         :- 'UnresolvedRelation [TDIAS], [], false\n         +- SubqueryAlias tweather\n            +- View (`TWEATHER`, [FECHA#82,DIA#83,TMax#84,HTMax#85,TMin#86,HTMin#87,TMed#88,HumMax#89,HumMin#90,HumMed#91,VelViento#92,DirViento#93,Rad#94,Precip#95,ETo#96])\n               +- Relation [FECHA#82,DIA#83,TMax#84,HTMax#85,TMin#86,HTMin#87,TMed#88,HumMax#89,HumMin#90,HumMed#91,VelViento#92,DirViento#93,Rad#94,Precip#95,ETo#96] csv\n"
          ]
        }
      ]
    }
  ]
}